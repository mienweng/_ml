# 第2章 模型與學習基礎

## 2-1 監督式學習

### 1. 編解、分類與預測（Encoding, Classification, and Prediction）
這些是 AI 中最基本的任務形式之一：

#### ▶ 編解（Encoding & Decoding）

* 編碼（Encoding）：將資料轉換為機器可理解的格式（如向量、數字），例如將文字轉成詞向量（Word Embedding）。
* 解碼（Decoding）：將機器的內部表示轉回人類可理解的形式，例如把模型產出的數字序列轉回文字句子。

應用：
* 在 NLP 中，編碼器-解碼器（Encoder-Decoder）架構常見於翻譯與摘要任務。
* 在自動生成任務（如圖像描述或語音合成）中也使用類似架構。

#### ▶ 分類（Classification）

* 將輸入資料分配至預定的類別中。
* 屬於**監督式學習**，需有標註資料（label）。

例子：
* 圖像分類（貓、狗）
* 郵件分類（垃圾郵件、正常郵件）

常見算法：
* Logistic Regression
* 支持向量機（SVM）
* 深度神經網絡（DNN）

#### ▶ 預測（Prediction）
* 不一定是分類，也可以是數值（回歸）、機率、時間序列等。
* 任務目標是對未來或未知資料做出推測。

例子：
* 股票價格預測
* 醫療風險評估
* 用戶行為預測（點擊率、購買可能性）

### 2. 損失函數與構造（Loss Functions and Their Design）
損失函數（Loss Function）是評估模型預測值與真實值差距的指標，是模型訓練的核心。

#### ▶ 目的
* 衡量模型的預測錯誤程度
* 為優化算法提供「方向與大小」的指引（例如梯度下降）

#### ▶ 常見損失函數

* **分類任務**：

  * 交叉熵損失（Cross-Entropy Loss）
* **回歸任務**：

  * 均方誤差（Mean Squared Error, MSE）
  * 絕對誤差（Mean Absolute Error, MAE）

#### ▶ 損失函數的選擇與設計

* 損失函數要根據任務性質選擇（分類 vs 回歸）
* 若任務有不平衡資料，可能需引入加權損失（Weighted Loss）或使用 Focal Loss
* 在特定應用（如強化學習、生成模型）中，常設計特殊損失結構

### 3. 模型訓練與合理性（Model Training and Soundness）
模型訓練是透過資料不斷更新參數，使模型能逐步學習並達到預測或分類的目標。

### ▶ 模型訓練流程

1. **初始化模型參數**
2. **前向傳播（Forward Pass）**：計算輸出與損失
3. **反向傳播（Backward Pass）**：根據損失計算梯度
4. **更新參數**：透過優化器（如 SGD、Adam）調整參數
5. **重複多輪（Epoch）直到收斂**

#### ▶ 訓練的合理性評估

* 使用訓練集和驗證集觀察是否過擬合
* 評估泛化能力：模型是否能對未看過的資料做出準確判斷
* 可視化損失曲線、準確率等指標以確保模型行為正常

#### ▶ 過擬合與欠擬合

* **過擬合**：模型記住訓練資料，無法泛化
* **欠擬合**：模型太簡單，無法捕捉資料中的模式

#### ▶ 提升訓練合理性的方法

* 正規化（L1/L2）
* Dropout、資料增強（Data Augmentation）
* 模型簡化或特徵選擇

## 2-2 非監督式學習

### 1. 群聚（Clustering）

**群聚**是一種無監督學習技術，目的是將資料自動分群，讓每一群內的資料相似度高、群與群之間差異大。

#### 常見群聚算法：

* **K-means**：

  * 將資料分為 K 個群，反覆調整中心點以最小化群內距離。
  * 優點：簡單快速；缺點：需事先指定 K。
* **DBSCAN（密度為本的空間聚類）**：

  * 根據資料密度進行群聚，可辨識非圓形群與雜訊。
* **層級式群聚（Hierarchical Clustering）**：

  * 以資料間距離建立樹狀結構，可視化群聚層級。

#### 應用領域：
* 市場區隔
* 基因資料分析
* 社群偵測與推薦系統

### 2. 附近探索與維度附近（Neighborhood Search & Dimensional Neighborhoods）
在高維度資料中，「誰是誰的鄰居」比你想像的更困難，但卻非常重要，因為許多無監督學習與分類方法依賴資料點間的相似度。

#### 附近探索（Nearest Neighbor Search）：

* 用於找出一筆資料最接近的其他點。
* 常見方法：

  * **K-最近鄰演算法（K-NN）**：基於距離（如歐氏距離）決定最鄰近的 K 個點。
  * **近似方法**（如 Ball Tree、KD-Tree）：加速搜尋速度，尤其適用於大量資料。

#### 維度附近（Dimensional Neighborhood）：

* 資料在高維空間中分布稀疏，「維度詛咒」讓距離失真。
* 解決方法：

  * **主成分分析（PCA）**：降維並保留資料主要變異方向。
  * **t-SNE / UMAP**：將高維資料壓縮至 2D/3D，保留局部鄰近關係，方便視覺化。

### 3. 缺資料模型與自然分類（Missing Data Models & Natural Clustering）
在現實資料中，常會遇到資料缺失（如欄位空白、記錄不完整）。如何處理缺資料，以及如何自動從資料中發現自然的分類結構，是無監督學習的重要議題。

#### 缺資料模型（Missing Data Models）：

* **處理方式**：

  * 刪除法（忽略缺值列）：風險是資訊損失。
  * 插補法（Imputation）：

    * 平均值、中位數填補
    * 利用相似樣本估算缺失值（如 K-NN）
    * 建立回歸模型預測缺值

* **進階技術**：

  * EM 演算法（Expectation-Maximization）：同時估計缺失值與模型參數
  * 多重插補（Multiple Imputation）：保留不確定性

#### 自然分類（Natural Clustering）：

* 不需人工標籤，資料本身即存在結構（例如客戶群、網路社群）。
* 透過聚類方法、自編碼器（Autoencoder）等方式學習潛在結構。

## 2-3 實踐式學習
實踐式學習是人工智能中一種模仿「試誤學習」的機制，讓代理人（Agent）透過在環境中的互動，學習達成目標的方法。這類學習方式特別適合動態決策、控制問題與策略優化，如遊戲、機器人、自駕車等領域。

### ▶ 環境、操作與回饋（Environment, Actions, and Rewards）

實踐式學習系統可抽象為一個「代理人與環境互動模型」：

| 元件                  | 說明                    |
| ------------------- | --------------------- |
| **環境（Environment）** | 問題所在的世界（如遊戲地圖、模擬系統）   |
| **代理人（Agent）**      | 做出決策的主體（如機器人、玩家）      |
| **狀態（State）**       | 描述環境目前狀況的資訊（如棋盤狀態、車速） |
| **行動（Action）**      | 代理人可採取的動作（如上下左右、加減速）  |
| **回饋（Reward）**      | 當前動作導致的立即性評價（如得分、懲罰）  |

> 學習目標：找到一種策略（Policy），讓代理人在長期中獲得最大總回饋。

### ▶ 實用範例：遊戲玩法成能（Game Playing & Skill Acquisition）
Q-Learning 常見於教機器學習玩遊戲。以下是簡化版本的應用案例：

#### 🎮 例子：格子世界（Grid World）

* 玩家在 5x5 格子中移動
* 起點：左上角；終點：右下角
* 每步扣 1 分，到達終點給 +10 分
* 目標：學會最短且得分最高的路徑

使用 Q-Learning 可逐步建立每個格子在各個方向的「最佳移動策略」。

#### 🤖 進階應用：

* **OpenAI Gym**：一個開源的 RL 測試平台，提供如經典 Atari 遊戲、CartPole、MountainCar 等模擬環境。
* **AlphaGo**：結合 Q-Learning、蒙地卡羅樹搜尋與深度學習，擊敗世界圍棋冠軍。
* **自動駕駛策略訓練**：讓車子學會避障、保持車道、超車等行為。